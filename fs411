#!/usr/bin/python3

## Input:
#    a full path to a folder containing Freesurfer output
#    This usu. looks something like "/home/mike/Brains/Rochester/R000001"
#

## Output:
#    Depending on the arguments provided at presentation, data derived
#    from the Freesurfer process, see arguments for details.
#    This is for extracting subject data like brain volumes, and for
#    metadata concerning the freesurfer process that generated them.

## We expect to find certain folders and files within any of these structures.
## Differences in filecounts here can indicate failure at different places
## through the pipeline.
#    ./bem             ?
#    ./label           ~69 files (.ctab, .annot, .label) containing labels for regions within images
#    ./mri             ~34 files, 24 output images with .mgz extensions
#         /orig        The mgz version of the input image
#         /transforms  Talairach transformations of the original
#    ./scripts         ~12 log files, useful for determining environment and commands issued
#    ./src             ?
#    ./stats           18 .stats files, quantitative results for volumes, areas, etc.
#    ./surf            ~70 files
#    ./tmp             ?
#    ./touch           Records of some commands in the pipeline
#    ./trash           ?


## Use others' code to make our lives easier
import sys  # To get command-line arguments
import argparse  # A framework for parsing command-line arguments
import os  # For digging through directories and files
import re  # Regular expression support
import datetime  # To handle dates and times
import csv  # To write the final csv mrs file easily
from collections import OrderedDict  # for named dictionaries of volumes and such
from datetime import date
from datetime import timedelta
from dateutil import parser
from dateutil.tz import gettz
import time
import glob  # for searching for original spgr files, fullname unknown

# Local file containing dictionaries of FreeSurfer variables
from mrs_dicts import *

##------------------------------------------------------------------------------
##-- Define constants and globals for later use
##------------------------------------------------------------------------------
TS_Format = "%a %b %d %H:%M:%S %Z %Y"

## Define any globals
neededfiles = {'log': '/scripts/recon-all.log',
               'stamp': '/scripts/build-stamp.txt',
               'cmd': '/scripts/recon-all.cmd',
               'aseg': '/stats/aseg.stats',
               'wmparc': '/stats/wmparc.stats',
               'aparc1l': '/stats/lh.aparc.stats',
               'aparc1r': '/stats/rh.aparc.stats',
               'aparc2l': '/stats/lh.aparc.a2009s.stats',
               'aparc2r': '/stats/rh.aparc.a2009s.stats',
               'aparc3l': '/stats/lh.aparc.DKTatlas40.stats',
               'aparc3r': '/stats/rh.aparc.DKTatlas40.stats'}
optionfiles = {'err': '/scripts/recon-all.error',
               'done': '/scripts/recon-all.done'}

subjectid = ""
ts_begin = datetime.datetime(1975, 3, 11, 0, 0, 0)
ts_end = ts_begin
tzinfos = {"CST": gettz("America/Chicago"), "CDT": gettz("America/Chicago"),
           "EST": gettz("America/New_York"), "EDT": gettz("America/New_York")}
StartedOver = ' '
left_hcv = ''
right_hcv = ''
hcvfile = 0


##------------------------------------------------------------------------------
##-- Define functions for later use
##------------------------------------------------------------------------------


##-- Sanity check... Do things look like we expect?
def input_ok(mypath):
    if os.path.isdir(mypath):
        partial = False
        full = True
        for fd, fv in neededfiles.items():
            if os.path.isfile(mypath + fv):
                partial = True
            else:
                full = False
        if full:
            # Only if the directory exists and has all expected files should we continue.
            return True
        elif partial:
            # This is actually OK for retrieving error status, but returning True means
            # it is now necessary to make every function robust to missing files.
            # print("I found \"{argdir}\". It has some freesurfer data, but not everything expected.".format(argdir=mypath))
            return True
        else:
            print("I found \"{argdir}\", but it does not appear to contain freesurfer data.".format(argdir=mypath))
            return False
    else:
        print("I cannot find the directory, \"{argdir}\".".format(argdir=mypath))
        return False
    print(
        "Damn! This is a bug in the \"input_ok()\" function. Execution should never have reached this point. I am not detecting files appropriately.")
    return False


## end input_ok function


##-- Extract the version information
def get_fsversion(verbosity="short"):
    stampfile = subjectroot + neededfiles['stamp']
    version_string = ""
    if not os.path.isfile(stampfile):
        return ("N/A")
    f = open(stampfile)
    for line in f:
        if '\n' == line[-1]:
            line = line[:-1]
        version_string = line
    f.close()
    if verbosity == "short":
        short_version_string = version_string[version_string.find("pub-v") + len("pub-v"):]
        return (short_version_string[:5])
    # else verbosity must be long
    return (version_string)


# / end get_fsversion() function


##-- Extract the elapsed time
def get_fstime(verbosity="short", timetype="elapsed", sparsity=False):
    cmdfile = subjectroot + neededfiles['cmd']
    if not os.path.isfile(cmdfile):
        return ("N/A")
    timestrings = []
    # tfstring="%a %b %d %X %Z %Y"
    first_time = datetime.datetime.now()
    last_time = datetime.datetime.now()
    this_time = datetime.datetime.now()
    reg_tstamp = re.compile(r'^#@# (?P<task>.*) (?P<stmp>\w{3}\s+\w+\s+\d+\s+\d\d:\d\d:\d\d\s+\w{3}\s+\d{4})')
    f = open(cmdfile)
    n = 0
    for line in f:
        mat_tstamp = reg_tstamp.match(line)
        if mat_tstamp:
            if n == 0:
                # first_time = datetime.datetime.strptime(mat_tstamp.group('stmp'), tfstring)
                first_time = parser.parse(mat_tstamp.group('stmp'), tzinfos=tzinfos)
                last_time = first_time
            n += 1
            # this_time = datetime.datetime.strptime(mat_tstamp.group('stmp'), tfstring)
            this_time = parser.parse(mat_tstamp.group('stmp'), tzinfos=tzinfos)
            td_small = this_time - last_time
            td_large = this_time - first_time
            if verbosity == "long":
                if sparsity:
                    timestrings.append("{:2} : {:5.0f}\n".format(n, td_small.total_seconds()))
                else:
                    timestrings.append("{:2} : {:28} : {} ({:5.0f}, {:5.0f})\n".format(n, mat_tstamp.group('task'),
                                                                                       mat_tstamp.group('stmp'),
                                                                                       td_small.total_seconds(),
                                                                                       td_large.total_seconds()))
            last_time = this_time
    f.close()
    if verbosity == "short":
        if sparsity:
            timestrings.append("{:5.0f}".format(td_large.total_seconds()))
        else:
            timestrings.append(
                "{:5.0f} seconds ({:2.1f} hrs)".format(td_large.total_seconds(), td_large.total_seconds() / 3600.0))
    if (timetype == "elapsed"):
        return (''.join(timestrings))
    if (timetype == "begin"):
        return (first_time.strftime("%Y-%m-%dT%H:%M:%S%z"))
    if (timetype == "end"):
        return (last_time.strftime("%Y-%m-%dT%H:%M:%S%z"))


## / end get_fselapsed() function


##-- Extract the hostname on which freesurfer was run
def get_fshostname():
    retval = ""
    if os.path.isfile(subjectroot + optionfiles["done"]):
        reg_machine = re.compile(r'.*HOST (?P<machine>\w*).*')
        f = open(subjectroot + optionfiles["done"])
        for line in f:
            mat_machine = reg_machine.match(line)
            if mat_machine:
                retval = mat_machine.group('machine')
        f.close()
    return (retval)


## / end get_fshostname() function


##-- Extract the username under which freesurfer was run
def get_fsusername():
    retval = ""
    if os.path.isfile(subjectroot + optionfiles["done"]):
        reg_username = re.compile(r'.*USER (?P<username>\w*).*')
        f = open(subjectroot + optionfiles["done"])
        for line in f:
            mat_username = reg_username.match(line)
            if mat_username:
                retval = mat_username.group('username')
        f.close()
    return (retval)


## / end get_fsusername() function


##-- Extract the date the original MRI was collected
#    This function is embarrassingly confined to GENOA data on one machine.
#    All other usage will get "NA" for their dates.
def get_scandate():
    retval = "NA"
    potential_files = glob.glob("/mri/gmbi.*.original/{0}/{0}*spgr.img".format(subjectid))
    if len(potential_files) != 1:
        return retval
    if os.path.isfile(potential_files[0]):
        # There is only one matching file, and it exists, so parse its name.
        reg_mridate = re.compile(r'.*_(?P<Y>\d\d\d\d)(?P<M>\d\d)(?P<D>\d\d)_spgr.img')
        mat_date = reg_mridate.match(potential_files[0])
        if mat_date:
            retval = "{Y}-{M}-{D}".format(Y=mat_date.group('Y'), M=mat_date.group('M'), D=mat_date.group('D'))
        else:
            print("{0} is not the expected filename pattern.".format(potential_files[0]))
    return (retval)


## / end get_fsusername() function


##-- Extract a volume or area
def get_fsvolume(whichnum=""):
    retval = ""
    # Merge dictionaries
    dict_whole = get_fsdata('aseg')
    dict_whole.update(get_fsdata('wmparc'))
    dict_whole.update(OrderedDict([(k + '_L', v) for k, v in get_fsdata('aparc1l').items()]))
    dict_whole.update(OrderedDict([(k + '_R', v) for k, v in get_fsdata('aparc1r').items()]))
    try:
        retval = dict_whole[whichnum]
    except KeyError:
        print("{0} cannot be found as a StructName in {1}".format(args.item, args.rootfolder))
        retval = "0.00"
    return (retval)


## / end get_fsvolume() function


##-- Retrieve data from FreeSurfer stats file
def get_fsdata(statsfile):
    mydict = {}
    if os.path.isfile(subjectroot + neededfiles[statsfile]):
        # Each stats file has two sections with data, a head section and
        # a body section (called CORT for aparcs). We need to check and mine both.
        reg_measurehead = re.compile(r'^#\s+Measure\s+[\w_\-]+,\s+(?P<Name>[\w_\-]+),.+?(?P<Volume>[\d\.]+),\s(?P<Dim>.+)$')
        reg_measurebody = re.compile(r'^\s*\d+\s+\d+\s+\d+\s+(?P<Volume>[\d\.]+)\s+(?P<Name>[\w_\-]+).+')
        reg_measurecort = re.compile(r'^(?P<Name>[\w_\-]+)\s+\d+\s+\d+\s+(?P<Volume>[\d\.]+).+')
        f = open(subjectroot + neededfiles[statsfile])
        for line in f:
            mat_measurehead = reg_measurehead.match(line)
            mat_measurebody = reg_measurebody.match(line)
            mat_measurecort = reg_measurecort.match(line)
            if mat_measurehead:
                # print("HEAD: {Name}={Volume}".format(Name=mat_measurehead.group('Name'), Volume = mat_measurehead.group('Volume')))
                mydict[mat_measurehead.group('Name')] = mat_measurehead.group('Volume')
            if mat_measurebody:
                # print("BODY: {Name}={Volume}".format(Name=mat_measurebody.group('Name'), Volume = mat_measurebody.group('Volume')))
                mydict[mat_measurebody.group('Name')] = mat_measurebody.group('Volume')
            if mat_measurecort:
                # print("CORT: {Name}={Volume}".format(Name=mat_measurecort.group('Name'), Volume = mat_measurecort.group('Volume')))
                mydict[mat_measurecort.group('Name')] = mat_measurecort.group('Volume')
        f.close()
    return (mydict)


##-- Write fields and values to csv file
def outputcsv(mynames, myvalues, myfile):
    file_is_new = not os.path.isfile(myfile)
    with open(myfile, 'a') as csvfile:
        mywriter = csv.writer(csvfile, delimiter=",", quotechar="\"", quoting=csv.QUOTE_MINIMAL)
        if file_is_new:
            mywriter.writerow(mynames)
        mywriter.writerow(myvalues)


##-- Save out the freesurfer data in tabular format for a single fs file
def write_fsfile(outfilename, fstype, whichVersion="600"):
    myname = outfilename
    if args.filetype == "all":
        if myname[-4:] != ".csv":
            myname = nyname + ".csv"
        if "mrs" in myname:
            myname = myname.replace("mrs", fstype)
        else:
            myname = outfilename[:-4] + '.' + fstype + outfilename[-4:]
    mydata = []
    rownames = []
    rowdata = []
    for n, d in get_metadict(subjectid, get_fstime("short", "end")).items():
        mydata.append((n, d))

    if fstype == 'aseg':
        aseg_dict = get_fsdata(fstype)
        for n in get_aseg_ordered_list(whichVersion):
            try:
                mydata.append((n, aseg_dict[n]))
            except KeyError:
                #print("   keyerror, can't find {0} in {1}".format(n, fstype))
                mydata.append((n, "NA"))
    elif fstype == 'wmparc':
        wmparc_dict = get_fsdata(fstype)
        for n in get_wmparc_ordered_list(whichVersion):
            try:
                mydata.append((n, wmparc_dict[n]))
            except KeyError:
                #print("   keyerror, can't find {0} in {1}".format(n, fstype))
                mydata.append((n, "NA"))
    elif fstype == 'aparc':
        aparc_dict = OrderedDict([(k + '_L', v) for k, v in get_fsdata('aparc1l').items()])
        aparc_dict.update(OrderedDict([(k + '_R', v) for k, v in get_fsdata('aparc1r').items()]))
        for n in get_aparc_ordered_list(whichVersion):
            n_L = "{0}_L".format(n)
            n_R = "{0}_R".format(n)
            try:
                mydata.append((n_L, aparc_dict[n_L]))
            except KeyError:
                #print("   keyerror, can't find {0} in {1}".format(n_L, fstype))
                mydata.append((n_L, "NA"))
            try:
                mydata.append((n_R, aparc_dict[n_R]))
            except KeyError:
                #print("   keyerror, can't find {0} in {1}".format(n_R, fstype))
                mydata.append((n_R, "NA"))

    for i in mydata:
        rownames.append(i[0])
        rowdata.append(i[1])

    print("Writing raw {ft} data for {sid} to {fp}".format(ft=fstype, sid=subjectid, fp=myname))
    outputcsv(rownames, rowdata, myname)


##-- Order the variables and their labels for an mrs-formatted file
def write_mrsfile(wholedict, csvmrsfile):
    myname = csvmrsfile
    if args.filetype == "all":
        # with "all", we are generating four uniquely named files from a single input file.
        if myname[-4:] != ".csv":
            myname = myname + ".csv"
        if "mrs" not in myname:
            myname = myname[:-4] + '.mrs.csv'
    rowNames = []
    rowData = []
    for n, d in get_mrsmeta_dict(subjectid, get_scandate(), get_fstime("short", "end")).items():
        rowNames.append(n)
        rowData.append(d)
    for n, d in get_mrs_dict().items():
        # each key-value pair looks like 'mrs#':'Name'
        # append the identifier (key) to the beginning of the mrs list
        rowNames.append(n)
        # and find the appropriate data value to match it
        if wholedict.get(d, "FAIL") != "FAIL":
            # Successful lookup of mrs number from FreeSurfer description
            rowData.append(wholedict.get(d, "FAIL"))
        else:
            if d == "Scandate_Month":
                rowData.append(get_scandate()[5:7])
            elif d == "Scandate_Day":
                rowData.append(get_scandate()[8:10])
            elif d == "Scandate_Year":
                rowData.append(get_scandate()[0:4])
            elif d == "Scandate_Time":
                rowData.append(get_scandate()[11:])
            elif d == "Analysisdate_Month":
                rowData.append(get_fstime("short", "end")[5:7])
            elif d == "Analysisdate_Day":
                rowData.append(get_fstime("short", "end")[8:10])
            elif d == "Analysisdate_Year":
                rowData.append(get_fstime("short", "end")[0:4])
            elif d == "Analysisdate_Time":
                rowData.append(get_fstime("short", "end")[11:])
            elif d == "ThalamusProper_T":
                # This exception is necessary to match the tweaked mrs variable names
                lval = float(wholedict.get('Left-Thalamus-Proper', "0.0"))  # Match at Left-***
                rval = float(wholedict.get('Right-Thalamus-Proper', "0.0"))  # Match at Right-***
                rowData.append("{:0.2f}".format(lval + rval))
            elif d == "Vetrical_T":
                # This exception adds up all ventricles and stores them under the mrs mis-spelling
                ventricles = (float(wholedict.get('Left-Lateral-Ventricle', "0.0")) +
                              float(wholedict.get('Left-Inf-Lat-Vent', "0.0")) +
                              float(wholedict.get('3rd-Ventricle', "0.0")) +
                              float(wholedict.get('4th-Ventricle', "0.0")) +
                              float(wholedict.get('Right-Lateral-Ventricle', "0.0")) +
                              float(wholedict.get('Right-Inf-Lat-Vent', "0.0")) +
                              float(wholedict.get('5th-Ventricle', "0.0")))
                rowData.append("{:0.2f}".format(ventricles))
            elif d[-2:] == "_T":
                # We have a "total" field, so we need to find the left and right fields to add for it
                if wholedict.get(d[:-2] + '_L', "FAIL") == "FAIL":
                    if wholedict.get('Left-' + d[:-2], "FAIL") == "FAIL":
                        # print("--== ERROR: {4}-{0}: {1} but no {2} or {3}. Setting it to 0.0".format(n, d, d[:-2] + '_L', 'Left-' + d[:-2], subjectid))
                        lval = 0.0  # No match, actual failure
                    else:
                        lval = float(wholedict.get('Left-' + d[:-2], "0.0"))  # Match at Left-***
                else:
                    lval = float(wholedict.get(d[:-2] + '_L', "0.0"))  # Match at ***_L
                if wholedict.get(d[:-2] + '_R', "FAIL") == "FAIL":
                    if wholedict.get('Right-' + d[:-2], "FAIL") == "FAIL":
                        # print("--== ERROR: {4}-{0}: {1} but no {2} or {3}. Setting it to 0.0".format(n, d, d[:-2] + '_R', 'Right-' + d[:-2], subjectid))
                        rval = 0.0  # No match, actual failure
                    else:
                        rval = float(wholedict.get('Right-' + d[:-2], "0.0"))  # Match at Right-***
                else:
                    rval = float(wholedict.get(d[:-2] + '_R', "0.0"))  # Match at ***_R
                rowData.append("{:0.2f}".format(lval + rval))
            elif n[-3:] == "MRS":
                rowData.append("NA")
            else:
                rowData.append("0.00")
    print("Writing standard mrs data for {sid} to {fp}".format(sid=subjectid, fp=myname))
    outputcsv(rowNames, rowData, myname)


##-- Print data differently depending on sparsity and verbosity
def output_item(the_item):
    if args.sparse:
        print(the_item)
    else:
        print("{sid} : {d}".format(sid=subjectid, d=the_item))


##------------------------------------------------------------------------------
##-- Define expected command-line arguments
##------------------------------------------------------------------------------


##-- Parse command-line arguments
argparser = argparse.ArgumentParser(description='Dig for information from FreeSurfer output')
argparser.add_argument('rootfolder', help='Provide a subject\'s root FreeSurfer output folder.')
argparser.add_argument('-s', '--sparse', action='store_true', help='Avoid printing the ID, keep it sparse.')
argparser.add_argument('-v', '--version_short', action='store_true', help='Output the dot version of FreeSurfer used.')
argparser.add_argument('-V', '--version_long', action='store_true', help='Output the full version of FreeSurfer used.')
argparser.add_argument('-t', '--time_short', action='store_true',
                       help='Output the total elapsed time of the whole run.')
argparser.add_argument('-T', '--time_long', action='store_true', help='Output the individual timings of each step.')
argparser.add_argument('-u', '--username', action='store_true', help='Output the user account running freesurfer.')
argparser.add_argument('-m', '--machine', action='store_true', help='Output the host on which this subject was run.')
argparser.add_argument('-c', '--completion', action='store_true', help='Output success or failure of the job.')
argparser.add_argument('-d', '--completedate', action='store_true',
                       help='Output the date FreeSurfer completed its processing.')
argparser.add_argument('-D', '--scandate', action='store_true',
                       help='Only works for GENOA naming of files, return date of MRI.')
argparser.add_argument('-i', '--item', help='Output the item specified, looked up by FreeSurfer StructName.')
argparser.add_argument('--filetype', help='aseg,aparc,wmparc,mrs or all , used with --outfile')
argparser.add_argument('--outfile', help='Generate a csv file with the name specified, default is filetype=all.')

args = argparser.parse_args()

subjectroot = os.path.abspath(args.rootfolder)

##-- Do we have a legitimate set of freesurfer data to work with?
if not input_ok(subjectroot):
    # Errors will be printed from input_ok
    sys.exit()

##------------------------------------------------------------------------------
##-- Finally, with functions and arguments ready, do our thing...
##------------------------------------------------------------------------------


##-- Start by determining the subject ID
reg_id = re.compile(r'.*(?P<sid>[A-Za-z][0-9]{6})')
mat_id = reg_id.match(subjectroot)
if mat_id:
    subjectid = mat_id.group('sid')
else:
    print("I can't determine a subject's ID from '{}'.".format(subjectroot))
    sys.exit()

##-- Deal with versioning
if args.version_short:
    output_item(get_fsversion(verbosity="short"))
elif args.version_long:
    output_item(get_fsversion(verbosity="long"))

##-- Deal with elapsed times
if args.time_short:
    output_item(get_fstime(verbosity="short", timetype="elapsed", sparsity=False))
elif args.time_long:
    output_item(get_fstime(verbosity="long", timetype="elapsed", sparsity=False))

##-- Determine the completion status
if args.completion:
    if os.path.isfile(subjectroot + optionfiles["err"]):
        if args.sparse:
            print("error   ")
        else:
            f = open(subjectroot + optionfiles["err"])
            lastline = ""
            for line in f:
                lastline = line
            f.close()
            print("{} : Error in '{}'".format(subjectid, lastline))
    if os.path.isfile(subjectroot + optionfiles["done"]):
        output_item("complete")

##-- Determine the user or hostname
if args.username:
    output_item(get_fsusername())

##-- Determine the machine name that ran FreeSurfer
if args.machine:
    output_item(get_fshostname())

##-- Determine the date FreeSurfer completed
if args.completedate:
    output_item(get_fstime(timetype='end'))

##-- Determine the date the MRI was done
if args.scandate:
    output_item(get_scandate())

##-- Look up item in FreeSurfer files
if args.item:
    if args.sparse:
        print(get_fsvolume(whichnum=args.item, sparsity=args.sparse))
    else:
        print("{sid} : {structname} = {structvalue}".format(sid=subjectid, structname=args.item,
                                                            structvalue=get_fsvolume(whichnum=args.item)))

##-- Generate whole csvs with bunches of data
if args.outfile:
    if args.filetype == "aseg" or args.filetype == "all":
        write_fsfile(args.outfile, 'aseg', get_fsversion(verbosity="short"))
    if args.filetype == "wmparc" or args.filetype == "all":
        write_fsfile(args.outfile, 'wmparc', get_fsversion(verbosity="short"))
    if args.filetype == "aparc" or args.filetype == "all":
        write_fsfile(args.outfile, 'aparc', get_fsversion(verbosity="short"))
    if args.filetype == "mrs" or args.filetype == "all":
        dict_whole = get_fsdata('aseg')
        dict_whole.update(get_fsdata('wmparc'))
        dict_whole.update(OrderedDict([(k + '_L', v) for k, v in get_fsdata('aparc1l').items()]))
        dict_whole.update(OrderedDict([(k + '_R', v) for k, v in get_fsdata('aparc1r').items()]))
        write_mrsfile(dict_whole, args.outfile)

# print("Done!")
